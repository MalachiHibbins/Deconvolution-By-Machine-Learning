
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Neural Networks for Image De-noising &#8212; Deconvolution Using Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Week7/MachineLearning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Deconvolution Using Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Deconvolution Using Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    De-convolution Project Lab Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 1 - PyTorch Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week1/1PytorchTutorial.html">PyTorch Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 2 - Deblurring Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week2/12DFowierTransforms.html">2D Fourier Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Week2/2ImageDebluring.html">Image Deblurring</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 3 and 4 - Richardson and Lucy Algorithm</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Week3/1richardsonlucy.html">Richardson-Lucy Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Week3/2richardsonlucy.html">Tuning the Richardson-Lucy Algorithm with 1D functions</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 5-8 - Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="MachineLearningFunction.html">Neural Networks for Image De-noising</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FWeek7/MachineLearning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Week7/MachineLearning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural Networks for Image De-noising</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theory">Theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-test-data">Generate test data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-data">Preparing the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-mlp-model">Defining the MLP model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-model">Initialize model</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-networks-for-image-de-noising">
<h1>Neural Networks for Image De-noising<a class="headerlink" href="#neural-networks-for-image-de-noising" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Understand how multi-layer neural networks function.</p></li>
<li><p>Learn about backpropagation and gradient descent algorithms.</p></li>
</ul>
<section id="theory">
<h2>Theory<a class="headerlink" href="#theory" title="Link to this heading">#</a></h2>
<p>Assume the noisy image can be written as a matrix <span class="math notranslate nohighlight">\(P_{i}\)</span> where each element represents the pixel intensity of the <span class="math notranslate nohighlight">\(i\)</span> th pixel. The goal is to train a neural network <span class="math notranslate nohighlight">\(f(P; K)\)</span> where <span class="math notranslate nohighlight">\(K\)</span> is the known convolution kernel to output a denoised image <span class="math notranslate nohighlight">\(Q_{i} = f(P; K)\)</span>. The nodes between the inputs and the outputs are the ‘hidden nodes’. All nodes have a value between 0 and 1.</p>
<p><img alt="image.png" src="Week7/attachment:image.png" /></p>
<p>The node <span class="math notranslate nohighlight">\(a^{(1)}_0\)</span> will have its value computed from a sum of the weighted inputs from the first layer.
$<span class="math notranslate nohighlight">\(a^{(1)}_0 = \sigma(b_0+\sum_{i} w_{i}P_{i})\)</span><span class="math notranslate nohighlight">\(
Where \)</span>\sigma<span class="math notranslate nohighlight">\( is the normalisation function given by \)</span>\sigma(x) = \frac{1}{1 + e^{-x}}<span class="math notranslate nohighlight">\(. \)</span>w_{i}<span class="math notranslate nohighlight">\( are the weights associated with each input pixel \)</span>P_{i}<span class="math notranslate nohighlight">\(, \)</span>b_0$ is the bias term.</p>
<p>The first next layer of hidden nodes can be calculated from:</p>
<div class="math notranslate nohighlight">
\[\mathbf{a}^{(i+1)} = \sigma(W^{(i)}\mathbf{a}^{(i)} + \mathbf{b}^{(i)})\]</div>
<p>Where <span class="math notranslate nohighlight">\(W^{(i)}\)</span> is the weight matrix and <span class="math notranslate nohighlight">\(\mathbf{b}^{(i)}\)</span> is the bias vector which transforms the <span class="math notranslate nohighlight">\(i\)</span> th layer into the <span class="math notranslate nohighlight">\(i+1\)</span> th layer. The values of <span class="math notranslate nohighlight">\(W^{(i)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}^{(i)}\)</span> are learned during training.</p>
</section>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>The cost function measures the success of the machine learning algorithm one way it could be defined is as follows:
$<span class="math notranslate nohighlight">\(C(\mathbf{w}) = \sum_i (\hat{p}_i - p_i)^2\)</span><span class="math notranslate nohighlight">\(
Where \)</span>\hat{p}_i<span class="math notranslate nohighlight">\( is the estimated state from the MLP and \)</span>p_i<span class="math notranslate nohighlight">\( is the known state. The vector \)</span>\mathbf{w}$ contains all the weights in the network. This sum is small when the network is close to being “correct”.</p>
<p>Using gradient decent can be used to find a local minima, an optimized solution. The vector <span class="math notranslate nohighlight">\(-\Delta C(\mathbf{w})\)</span> points in the direction of steepest decent.</p>
</section>
<section id="generate-test-data">
<h2>Generate test data<a class="headerlink" href="#generate-test-data" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">ImageDebluring</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>


<span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">step_function</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">amplitude</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">y_intercept</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">x_offset</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">steps</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">amplitude</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">xs</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">period</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">xs</span><span class="p">))</span> <span class="o">+</span> <span class="n">y_intercept</span>
    <span class="k">return</span> <span class="n">ys</span>

<span class="k">def</span> <span class="nf">generate_random_function</span><span class="p">():</span>
    <span class="n">period</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mi">5</span>
    <span class="n">amplitude</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="c1">#amplitude = 0.5</span>
    <span class="n">y_intercept</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span>
    <span class="n">x_offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span>
    <span class="k">return</span> <span class="n">step_function</span><span class="p">(</span><span class="n">period</span><span class="p">,</span> <span class="n">amplitude</span><span class="p">,</span> <span class="n">y_intercept</span><span class="p">,</span> <span class="n">x_offset</span><span class="o">=</span><span class="n">x_offset</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">generate_random_function</span><span class="p">()</span>
    <span class="n">degraded_data</span> <span class="o">=</span> <span class="n">degrade_image_1D</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">noise_scale</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">gaussian_normalised_kernel_1D</span><span class="p">(</span><span class="mi">61</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;replicate&#39;</span><span class="p">)</span>
    <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">degraded_data</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    
<span class="c1"># Make tensors</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">ImageDebluring</span> <span class="kn">import</span> <span class="o">*</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;torch&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="preparing-the-data">
<h2>Preparing the data<a class="headerlink" href="#preparing-the-data" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1"># Create training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Standardize the data</span>
<span class="n">scaler_x</span><span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler_x</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler_x</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">y_train_scaled</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test_scaled</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Convert to PyTorch tensors</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_test_scaled</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span>  <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">),</span>  <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train_scaled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train_scaled</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train_scaled</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test_scaled</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_test_scaled</span><span class="p">,</span>  <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test_scaled</span><span class="p">),</span>  <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-the-mlp-model">
<h2>Defining the MLP model<a class="headerlink" href="#defining-the-mlp-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>input size for this dataset is 1000 (number of points in the 1D signal)</p></li>
<li><p>hidden size can be adjusted, start with 128</p></li>
<li><p>output size is also 1000 (number of points in the 1D signal)</p></li>
<li><p>requires_grad=True to enable backpropagation</p></li>
</ul>
</section>
<section id="forward-pass">
<h2>Forward Pass<a class="headerlink" href="#forward-pass" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Linear transformation of inputs <span class="math notranslate nohighlight">\(z^{(1)} = XW^{(1)} + b^{(1)}\)</span></p></li>
<li><p>Apply activation function <span class="math notranslate nohighlight">\(a^{(1)} = \sigma(z^{(1)})\)</span> to node values</p></li>
<li><p>Linear transformation of hidden layer <span class="math notranslate nohighlight">\(z^{(2)} = a^{(1)}W^{(2)} + b^{(2)}\)</span></p></li>
<li><p>Apply activation function <span class="math notranslate nohighlight">\(a^{(2)} = \sigma(z^{(2)})\)</span> to get final output</p></li>
</ul>
</section>
<section id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">backward</span></code> updates the weights and the baises</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code> the number of times the model sees the entire dataset</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lr</span></code> hyperparameter that controls the step size for weighted updates</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss</span></code> the MSE</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">m</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="p">)</span> <span class="c1"># applies sigmoid activation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">output</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dz2</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">dW2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz2</span><span class="p">)</span>
        <span class="n">db2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        
        <span class="n">da1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dz1</span> <span class="o">=</span> <span class="n">da1</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">))</span>
        <span class="n">dW1</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">dz1</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        <span class="n">db1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dW1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">db1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dW2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">db2</span>
            
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="c1"># Compute loss using MSE</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">output</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="c1"># Update weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span>
        
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialize-model">
<h2>Initialize model<a class="headerlink" href="#initialize-model" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">*</span> <span class="mi">4</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 0.2826111316680908
Epoch 1, Loss: 0.3046635687351227
Epoch 2, Loss: 0.22058650851249695
Epoch 3, Loss: 0.3641345202922821
Epoch 4, Loss: 0.22794358432292938
Epoch 5, Loss: 0.22127588093280792
Epoch 6, Loss: 0.3512468636035919
Epoch 7, Loss: 0.22595803439617157
Epoch 8, Loss: 0.25563618540763855
Epoch 9, Loss: 0.290716290473938
Epoch 10, Loss: 0.22463206946849823
Epoch 11, Loss: 0.26276642084121704
Epoch 12, Loss: 0.23498019576072693
Epoch 13, Loss: 0.2632187604904175
Epoch 14, Loss: 0.21786652505397797
Epoch 15, Loss: 0.22804927825927734
Epoch 16, Loss: 0.24909287691116333
Epoch 17, Loss: 0.23620811104774475
Epoch 18, Loss: 0.21807365119457245
Epoch 19, Loss: 0.22538995742797852
Epoch 20, Loss: 0.23995348811149597
Epoch 21, Loss: 0.23059594631195068
Epoch 22, Loss: 0.2083396315574646
Epoch 23, Loss: 0.22705872356891632
Epoch 24, Loss: 0.2365685999393463
Epoch 25, Loss: 0.22025153040885925
Epoch 26, Loss: 0.2076926827430725
Epoch 27, Loss: 0.233915314078331
Epoch 28, Loss: 0.23110757768154144
Epoch 29, Loss: 0.2172236293554306
Epoch 30, Loss: 0.2148207128047943
Epoch 31, Loss: 0.22625768184661865
Epoch 32, Loss: 0.22183345258235931
Epoch 33, Loss: 0.2226855456829071
Epoch 34, Loss: 0.2163131982088089
Epoch 35, Loss: 0.22146406769752502
Epoch 36, Loss: 0.22515888512134552
Epoch 37, Loss: 0.225276917219162
Epoch 38, Loss: 0.2102350890636444
Epoch 39, Loss: 0.2218179851770401
Epoch 40, Loss: 0.22866785526275635
Epoch 41, Loss: 0.2165638655424118
Epoch 42, Loss: 0.21346214413642883
Epoch 43, Loss: 0.2275482714176178
Epoch 44, Loss: 0.22086554765701294
Epoch 45, Loss: 0.21912652254104614
Epoch 46, Loss: 0.21314965188503265
Epoch 47, Loss: 0.22230300307273865
Epoch 48, Loss: 0.22168099880218506
Epoch 49, Loss: 0.21850159764289856
Epoch 50, Loss: 0.2151239663362503
Epoch 51, Loss: 0.22141523659229279
Epoch 52, Loss: 0.2199219912290573
Epoch 53, Loss: 0.22015245258808136
Epoch 54, Loss: 0.2113572359085083
Epoch 55, Loss: 0.223221555352211
Epoch 56, Loss: 0.22305700182914734
Epoch 57, Loss: 0.21365772187709808
Epoch 58, Loss: 0.21370847523212433
Epoch 59, Loss: 0.2234048992395401
Epoch 60, Loss: 0.21730071306228638
Epoch 61, Loss: 0.2215539664030075
Epoch 62, Loss: 0.2103716880083084
Epoch 63, Loss: 0.22072727978229523
Epoch 64, Loss: 0.22709906101226807
Epoch 65, Loss: 0.20558950304985046
Epoch 66, Loss: 0.22258320450782776
Epoch 67, Loss: 0.23461376130580902
Epoch 68, Loss: 0.20265261828899384
Epoch 69, Loss: 0.21768631041049957
Epoch 70, Loss: 0.21776537597179413
Epoch 71, Loss: 0.21388550102710724
Epoch 72, Loss: 0.21229159832000732
Epoch 73, Loss: 0.21902106702327728
Epoch 74, Loss: 0.21283139288425446
Epoch 75, Loss: 0.21552345156669617
Epoch 76, Loss: 0.21768692135810852
Epoch 77, Loss: 0.2151240110397339
Epoch 78, Loss: 0.209723100066185
Epoch 79, Loss: 0.22496013343334198
Epoch 80, Loss: 0.21452203392982483
Epoch 81, Loss: 0.21318481862545013
Epoch 82, Loss: 0.22084811329841614
Epoch 83, Loss: 0.21578054130077362
Epoch 84, Loss: 0.2147040069103241
Epoch 85, Loss: 0.22181718051433563
Epoch 86, Loss: 0.2001131922006607
Epoch 87, Loss: 0.22957222163677216
Epoch 88, Loss: 0.21648091077804565
Epoch 89, Loss: 0.20211100578308105
Epoch 90, Loss: 0.21494823694229126
Epoch 91, Loss: 0.23038718104362488
Epoch 92, Loss: 0.1915392428636551
Epoch 93, Loss: 0.23897302150726318
Epoch 94, Loss: 0.2230515480041504
Epoch 95, Loss: 0.20690913498401642
Epoch 96, Loss: 0.2005631923675537
Epoch 97, Loss: 0.23161213099956512
Epoch 98, Loss: 0.2136792242527008
Epoch 99, Loss: 0.20129145681858063
Epoch 100, Loss: 0.2070194035768509
Epoch 101, Loss: 0.22929422557353973
Epoch 102, Loss: 0.21188896894454956
Epoch 103, Loss: 0.20488031208515167
Epoch 104, Loss: 0.20136378705501556
Epoch 105, Loss: 0.23668447136878967
Epoch 106, Loss: 0.21018873155117035
Epoch 107, Loss: 0.20809400081634521
Epoch 108, Loss: 0.21128718554973602
Epoch 109, Loss: 0.22850389778614044
Epoch 110, Loss: 0.20966149866580963
Epoch 111, Loss: 0.20583176612854004
Epoch 112, Loss: 0.21239063143730164
Epoch 113, Loss: 0.23078425228595734
Epoch 114, Loss: 0.1992003619670868
Epoch 115, Loss: 0.2178700566291809
Epoch 116, Loss: 0.2024301141500473
Epoch 117, Loss: 0.21372802555561066
Epoch 118, Loss: 0.2248535007238388
Epoch 119, Loss: 0.20776702463626862
Epoch 120, Loss: 0.19137372076511383
Epoch 121, Loss: 0.22862905263900757
Epoch 122, Loss: 0.21762634813785553
Epoch 123, Loss: 0.2007087767124176
Epoch 124, Loss: 0.19660912454128265
Epoch 125, Loss: 0.2330567091703415
Epoch 126, Loss: 0.21575088798999786
Epoch 127, Loss: 0.20525893568992615
Epoch 128, Loss: 0.20130044221878052
Epoch 129, Loss: 0.23110373318195343
Epoch 130, Loss: 0.21491779386997223
Epoch 131, Loss: 0.20174096524715424
Epoch 132, Loss: 0.21050654351711273
Epoch 133, Loss: 0.22703464329242706
Epoch 134, Loss: 0.20605047047138214
Epoch 135, Loss: 0.21022123098373413
Epoch 136, Loss: 0.19284240901470184
Epoch 137, Loss: 0.22122982144355774
Epoch 138, Loss: 0.22429588437080383
Epoch 139, Loss: 0.19113850593566895
Epoch 140, Loss: 0.19940270483493805
Epoch 141, Loss: 0.21775177121162415
Epoch 142, Loss: 0.20312228798866272
Epoch 143, Loss: 0.2183867245912552
Epoch 144, Loss: 0.18247772753238678
Epoch 145, Loss: 0.23422156274318695
Epoch 146, Loss: 0.20408669114112854
Epoch 147, Loss: 0.2142924666404724
Epoch 148, Loss: 0.20267850160598755
Epoch 149, Loss: 0.21728424727916718
Epoch 150, Loss: 0.1957990527153015
Epoch 151, Loss: 0.2249063402414322
Epoch 152, Loss: 0.1954803615808487
Epoch 153, Loss: 0.22417384386062622
Epoch 154, Loss: 0.20076437294483185
Epoch 155, Loss: 0.21616527438163757
Epoch 156, Loss: 0.20781831443309784
Epoch 157, Loss: 0.22347970306873322
Epoch 158, Loss: 0.18838337063789368
Epoch 159, Loss: 0.22180059552192688
Epoch 160, Loss: 0.2063804417848587
Epoch 161, Loss: 0.21974647045135498
Epoch 162, Loss: 0.20049001276493073
Epoch 163, Loss: 0.2215646207332611
Epoch 164, Loss: 0.20715820789337158
Epoch 165, Loss: 0.22088690102100372
Epoch 166, Loss: 0.19211219251155853
Epoch 167, Loss: 0.2217852622270584
Epoch 168, Loss: 0.2055218368768692
Epoch 169, Loss: 0.22219260036945343
Epoch 170, Loss: 0.1979091614484787
Epoch 171, Loss: 0.22236008942127228
Epoch 172, Loss: 0.20714996755123138
Epoch 173, Loss: 0.22327831387519836
Epoch 174, Loss: 0.19737277925014496
Epoch 175, Loss: 0.22045250236988068
Epoch 176, Loss: 0.20572419464588165
Epoch 177, Loss: 0.22413192689418793
Epoch 178, Loss: 0.19485074281692505
Epoch 179, Loss: 0.21947787702083588
Epoch 180, Loss: 0.20650674402713776
Epoch 181, Loss: 0.22402413189411163
Epoch 182, Loss: 0.19830824434757233
Epoch 183, Loss: 0.21991653740406036
Epoch 184, Loss: 0.2045995593070984
Epoch 185, Loss: 0.2252780646085739
Epoch 186, Loss: 0.19851577281951904
Epoch 187, Loss: 0.21472428739070892
Epoch 188, Loss: 0.20737609267234802
Epoch 189, Loss: 0.221964031457901
Epoch 190, Loss: 0.19463182985782623
Epoch 191, Loss: 0.22153803706169128
Epoch 192, Loss: 0.20426829159259796
Epoch 193, Loss: 0.22112776339054108
Epoch 194, Loss: 0.19274000823497772
Epoch 195, Loss: 0.2217458039522171
Epoch 196, Loss: 0.20496436953544617
Epoch 197, Loss: 0.21711264550685883
Epoch 198, Loss: 0.19723375141620636
Epoch 199, Loss: 0.2180679738521576
Epoch 200, Loss: 0.2045244723558426
Epoch 201, Loss: 0.2248484045267105
Epoch 202, Loss: 0.19206920266151428
Epoch 203, Loss: 0.2176939696073532
Epoch 204, Loss: 0.20627643167972565
Epoch 205, Loss: 0.21954728662967682
Epoch 206, Loss: 0.1984843611717224
Epoch 207, Loss: 0.20996715128421783
Epoch 208, Loss: 0.20420046150684357
Epoch 209, Loss: 0.2271772176027298
Epoch 210, Loss: 0.19075225293636322
Epoch 211, Loss: 0.2201979011297226
Epoch 212, Loss: 0.18851521611213684
Epoch 213, Loss: 0.2307746708393097
Epoch 214, Loss: 0.21316176652908325
Epoch 215, Loss: 0.20564860105514526
Epoch 216, Loss: 0.18349209427833557
Epoch 217, Loss: 0.22703051567077637
Epoch 218, Loss: 0.2045145034790039
Epoch 219, Loss: 0.21482188999652863
Epoch 220, Loss: 0.18747997283935547
Epoch 221, Loss: 0.2329140305519104
Epoch 222, Loss: 0.20599208772182465
Epoch 223, Loss: 0.21351300179958344
Epoch 224, Loss: 0.19155965745449066
Epoch 225, Loss: 0.2251794934272766
Epoch 226, Loss: 0.19428744912147522
Epoch 227, Loss: 0.22077666223049164
Epoch 228, Loss: 0.19242611527442932
Epoch 229, Loss: 0.22774073481559753
Epoch 230, Loss: 0.1965881735086441
Epoch 231, Loss: 0.22035403549671173
Epoch 232, Loss: 0.19860947132110596
Epoch 233, Loss: 0.22021009027957916
Epoch 234, Loss: 0.19040974974632263
Epoch 235, Loss: 0.21989189088344574
Epoch 236, Loss: 0.19601359963417053
Epoch 237, Loss: 0.22689053416252136
Epoch 238, Loss: 0.19507157802581787
Epoch 239, Loss: 0.22094643115997314
Epoch 240, Loss: 0.20097412168979645
Epoch 241, Loss: 0.22001242637634277
Epoch 242, Loss: 0.19541451334953308
Epoch 243, Loss: 0.21426475048065186
Epoch 244, Loss: 0.20032328367233276
Epoch 245, Loss: 0.22139762341976166
Epoch 246, Loss: 0.19311384856700897
Epoch 247, Loss: 0.21651574969291687
Epoch 248, Loss: 0.19751222431659698
Epoch 249, Loss: 0.2275783121585846
Epoch 250, Loss: 0.19608667492866516
Epoch 251, Loss: 0.21307270228862762
Epoch 252, Loss: 0.20006625354290009
Epoch 253, Loss: 0.22117631137371063
Epoch 254, Loss: 0.19597356021404266
Epoch 255, Loss: 0.21083052456378937
Epoch 256, Loss: 0.1931653916835785
Epoch 257, Loss: 0.22765569388866425
Epoch 258, Loss: 0.19784189760684967
Epoch 259, Loss: 0.2141532450914383
Epoch 260, Loss: 0.19743293523788452
Epoch 261, Loss: 0.22270362079143524
Epoch 262, Loss: 0.19873543083667755
Epoch 263, Loss: 0.2083028107881546
Epoch 264, Loss: 0.19637101888656616
Epoch 265, Loss: 0.22506794333457947
Epoch 266, Loss: 0.19447246193885803
Epoch 267, Loss: 0.2160252183675766
Epoch 268, Loss: 0.19267097115516663
Epoch 269, Loss: 0.22455868124961853
Epoch 270, Loss: 0.20357665419578552
Epoch 271, Loss: 0.2039404958486557
Epoch 272, Loss: 0.19963526725769043
Epoch 273, Loss: 0.21734027564525604
Epoch 274, Loss: 0.19606496393680573
Epoch 275, Loss: 0.2169746607542038
Epoch 276, Loss: 0.19024506211280823
Epoch 277, Loss: 0.23494279384613037
Epoch 278, Loss: 0.1921338140964508
Epoch 279, Loss: 0.2134484350681305
Epoch 280, Loss: 0.21311065554618835
Epoch 281, Loss: 0.21426917612552643
Epoch 282, Loss: 0.18560129404067993
Epoch 283, Loss: 0.20739278197288513
Epoch 284, Loss: 0.20641468465328217
Epoch 285, Loss: 0.22304341197013855
Epoch 286, Loss: 0.18747402727603912
Epoch 287, Loss: 0.22346504032611847
Epoch 288, Loss: 0.20495562255382538
Epoch 289, Loss: 0.21431751549243927
Epoch 290, Loss: 0.19480328261852264
Epoch 291, Loss: 0.20930521190166473
Epoch 292, Loss: 0.2038368582725525
Epoch 293, Loss: 0.21262003481388092
Epoch 294, Loss: 0.1951410323381424
Epoch 295, Loss: 0.21463608741760254
Epoch 296, Loss: 0.1958613097667694
Epoch 297, Loss: 0.22798876464366913
Epoch 298, Loss: 0.19643540680408478
Epoch 299, Loss: 0.20560148358345032
Epoch 300, Loss: 0.20589865744113922
Epoch 301, Loss: 0.22260265052318573
Epoch 302, Loss: 0.2021351307630539
Epoch 303, Loss: 0.1986115723848343
Epoch 304, Loss: 0.19454964995384216
Epoch 305, Loss: 0.23258091509342194
Epoch 306, Loss: 0.19256502389907837
Epoch 307, Loss: 0.21966487169265747
Epoch 308, Loss: 0.19772422313690186
Epoch 309, Loss: 0.22656086087226868
Epoch 310, Loss: 0.20964574813842773
Epoch 311, Loss: 0.20250675082206726
Epoch 312, Loss: 0.19562043249607086
Epoch 313, Loss: 0.21647194027900696
Epoch 314, Loss: 0.2033899426460266
Epoch 315, Loss: 0.22140346467494965
Epoch 316, Loss: 0.18223565816879272
Epoch 317, Loss: 0.24154826998710632
Epoch 318, Loss: 0.19488684833049774
Epoch 319, Loss: 0.2102350890636444
Epoch 320, Loss: 0.2052619308233261
Epoch 321, Loss: 0.22064943611621857
Epoch 322, Loss: 0.2001074254512787
Epoch 323, Loss: 0.2095518559217453
Epoch 324, Loss: 0.19996698200702667
Epoch 325, Loss: 0.22219198942184448
Epoch 326, Loss: 0.1870499551296234
Epoch 327, Loss: 0.23393192887306213
Epoch 328, Loss: 0.1939999908208847
Epoch 329, Loss: 0.22098103165626526
Epoch 330, Loss: 0.20348869264125824
Epoch 331, Loss: 0.20847803354263306
Epoch 332, Loss: 0.2075853943824768
Epoch 333, Loss: 0.21496185660362244
Epoch 334, Loss: 0.19420018792152405
Epoch 335, Loss: 0.2151087075471878
Epoch 336, Loss: 0.19155126810073853
Epoch 337, Loss: 0.2450731247663498
Epoch 338, Loss: 0.19043196737766266
Epoch 339, Loss: 0.21034488081932068
Epoch 340, Loss: 0.2052488476037979
Epoch 341, Loss: 0.22209499776363373
Epoch 342, Loss: 0.20691686868667603
Epoch 343, Loss: 0.2037705034017563
Epoch 344, Loss: 0.19233907759189606
Epoch 345, Loss: 0.2305000275373459
Epoch 346, Loss: 0.1891019642353058
Epoch 347, Loss: 0.23697206377983093
Epoch 348, Loss: 0.1929093897342682
Epoch 349, Loss: 0.22193174064159393
Epoch 350, Loss: 0.2084701508283615
Epoch 351, Loss: 0.2096310555934906
Epoch 352, Loss: 0.1992979496717453
Epoch 353, Loss: 0.21942587196826935
Epoch 354, Loss: 0.1995314359664917
Epoch 355, Loss: 0.21836227178573608
Epoch 356, Loss: 0.18332721292972565
Epoch 357, Loss: 0.24371396005153656
Epoch 358, Loss: 0.19425491988658905
Epoch 359, Loss: 0.2114981710910797
Epoch 360, Loss: 0.20897062122821808
Epoch 361, Loss: 0.22037485241889954
Epoch 362, Loss: 0.20044659078121185
Epoch 363, Loss: 0.20988024771213531
Epoch 364, Loss: 0.19797609746456146
Epoch 365, Loss: 0.22700722515583038
Epoch 366, Loss: 0.18756003677845
Epoch 367, Loss: 0.2320706695318222
Epoch 368, Loss: 0.19440729916095734
Epoch 369, Loss: 0.22067387402057648
Epoch 370, Loss: 0.2091519683599472
Epoch 371, Loss: 0.20703819394111633
Epoch 372, Loss: 0.20027244091033936
Epoch 373, Loss: 0.21839743852615356
Epoch 374, Loss: 0.19893409311771393
Epoch 375, Loss: 0.21823719143867493
Epoch 376, Loss: 0.18615026772022247
Epoch 377, Loss: 0.24363268911838531
Epoch 378, Loss: 0.19476643204689026
Epoch 379, Loss: 0.2107735574245453
Epoch 380, Loss: 0.20347362756729126
Epoch 381, Loss: 0.22361965477466583
Epoch 382, Loss: 0.1996881514787674
Epoch 383, Loss: 0.20759212970733643
Epoch 384, Loss: 0.19558893144130707
Epoch 385, Loss: 0.2207876294851303
Epoch 386, Loss: 0.19096267223358154
Epoch 387, Loss: 0.2255306839942932
Epoch 388, Loss: 0.19385750591754913
Epoch 389, Loss: 0.21751181781291962
Epoch 390, Loss: 0.20316626131534576
Epoch 391, Loss: 0.2124253809452057
Epoch 392, Loss: 0.19553108513355255
Epoch 393, Loss: 0.21162599325180054
Epoch 394, Loss: 0.1953105330467224
Epoch 395, Loss: 0.21635572612285614
Epoch 396, Loss: 0.19747643172740936
Epoch 397, Loss: 0.22189339995384216
Epoch 398, Loss: 0.19355319440364838
Epoch 399, Loss: 0.20933403074741364
Epoch 400, Loss: 0.20190972089767456
Epoch 401, Loss: 0.22046197950839996
Epoch 402, Loss: 0.1972547471523285
Epoch 403, Loss: 0.20671208202838898
Epoch 404, Loss: 0.1952342689037323
Epoch 405, Loss: 0.22007381916046143
Epoch 406, Loss: 0.19482649862766266
Epoch 407, Loss: 0.216475248336792
Epoch 408, Loss: 0.1943562775850296
Epoch 409, Loss: 0.22347500920295715
Epoch 410, Loss: 0.19949837028980255
Epoch 411, Loss: 0.20574301481246948
Epoch 412, Loss: 0.1970824897289276
Epoch 413, Loss: 0.214102640748024
Epoch 414, Loss: 0.19897328317165375
Epoch 415, Loss: 0.20626819133758545
Epoch 416, Loss: 0.19152066111564636
Epoch 417, Loss: 0.22298568487167358
Epoch 418, Loss: 0.18896709382534027
Epoch 419, Loss: 0.21528401970863342
Epoch 420, Loss: 0.1961013525724411
Epoch 421, Loss: 0.21489961445331573
Epoch 422, Loss: 0.19829829037189484
Epoch 423, Loss: 0.20229023694992065
Epoch 424, Loss: 0.20289266109466553
Epoch 425, Loss: 0.20452505350112915
Epoch 426, Loss: 0.18972253799438477
Epoch 427, Loss: 0.2156079262495041
Epoch 428, Loss: 0.19228778779506683
Epoch 429, Loss: 0.22203031182289124
Epoch 430, Loss: 0.1882171630859375
Epoch 431, Loss: 0.21341633796691895
Epoch 432, Loss: 0.19945526123046875
Epoch 433, Loss: 0.20639465749263763
Epoch 434, Loss: 0.19384481012821198
Epoch 435, Loss: 0.2006281316280365
Epoch 436, Loss: 0.19446343183517456
Epoch 437, Loss: 0.2145296037197113
Epoch 438, Loss: 0.1893160492181778
Epoch 439, Loss: 0.22643490135669708
Epoch 440, Loss: 0.18239909410476685
Epoch 441, Loss: 0.22214147448539734
Epoch 442, Loss: 0.19555681943893433
Epoch 443, Loss: 0.1960977464914322
Epoch 444, Loss: 0.196868896484375
Epoch 445, Loss: 0.20126046240329742
Epoch 446, Loss: 0.19622759521007538
Epoch 447, Loss: 0.2125055193901062
Epoch 448, Loss: 0.18241547048091888
Epoch 449, Loss: 0.2441147118806839
Epoch 450, Loss: 0.18642571568489075
Epoch 451, Loss: 0.21584409475326538
Epoch 452, Loss: 0.1897962987422943
Epoch 453, Loss: 0.21155822277069092
Epoch 454, Loss: 0.2066507488489151
Epoch 455, Loss: 0.19897478818893433
Epoch 456, Loss: 0.19025860726833344
Epoch 457, Loss: 0.21007667481899261
Epoch 458, Loss: 0.1969451904296875
Epoch 459, Loss: 0.2171856313943863
Epoch 460, Loss: 0.18626874685287476
Epoch 461, Loss: 0.22012440860271454
Epoch 462, Loss: 0.18730996549129486
Epoch 463, Loss: 0.21445976197719574
Epoch 464, Loss: 0.2005392462015152
Epoch 465, Loss: 0.2043105661869049
Epoch 466, Loss: 0.20143985748291016
Epoch 467, Loss: 0.1973390132188797
Epoch 468, Loss: 0.2011687308549881
Epoch 469, Loss: 0.20664134621620178
Epoch 470, Loss: 0.19903886318206787
Epoch 471, Loss: 0.2070200890302658
Epoch 472, Loss: 0.18374571204185486
Epoch 473, Loss: 0.22927463054656982
Epoch 474, Loss: 0.18661510944366455
Epoch 475, Loss: 0.2136632800102234
Epoch 476, Loss: 0.19676344096660614
Epoch 477, Loss: 0.212212473154068
Epoch 478, Loss: 0.19943608343601227
Epoch 479, Loss: 0.19107253849506378
Epoch 480, Loss: 0.20366999506950378
Epoch 481, Loss: 0.2101362645626068
Epoch 482, Loss: 0.19129271805286407
Epoch 483, Loss: 0.21115213632583618
Epoch 484, Loss: 0.18512943387031555
Epoch 485, Loss: 0.23066703975200653
Epoch 486, Loss: 0.18471458554267883
Epoch 487, Loss: 0.21444255113601685
Epoch 488, Loss: 0.19437208771705627
Epoch 489, Loss: 0.20209306478500366
Epoch 490, Loss: 0.20165875554084778
Epoch 491, Loss: 0.19201739132404327
Epoch 492, Loss: 0.1929394006729126
Epoch 493, Loss: 0.21508021652698517
Epoch 494, Loss: 0.19826531410217285
Epoch 495, Loss: 0.21020449697971344
Epoch 496, Loss: 0.18385092914104462
Epoch 497, Loss: 0.22607870399951935
Epoch 498, Loss: 0.18810181319713593
Epoch 499, Loss: 0.20772628486156464
Epoch 500, Loss: 0.2004765272140503
Epoch 501, Loss: 0.20424239337444305
Epoch 502, Loss: 0.1975405067205429
Epoch 503, Loss: 0.19878053665161133
Epoch 504, Loss: 0.20130382478237152
Epoch 505, Loss: 0.20610734820365906
Epoch 506, Loss: 0.19528871774673462
Epoch 507, Loss: 0.2130400538444519
Epoch 508, Loss: 0.17860861122608185
Epoch 509, Loss: 0.2311987727880478
Epoch 510, Loss: 0.19116295874118805
Epoch 511, Loss: 0.21100978553295135
Epoch 512, Loss: 0.19096598029136658
Epoch 513, Loss: 0.2028329074382782
Epoch 514, Loss: 0.2003050595521927
Epoch 515, Loss: 0.19291618466377258
Epoch 516, Loss: 0.1970711052417755
Epoch 517, Loss: 0.20619918406009674
Epoch 518, Loss: 0.20409634709358215
Epoch 519, Loss: 0.20343680679798126
Epoch 520, Loss: 0.18375419080257416
Epoch 521, Loss: 0.2205352485179901
Epoch 522, Loss: 0.18365636467933655
Epoch 523, Loss: 0.2139422595500946
Epoch 524, Loss: 0.19152458012104034
Epoch 525, Loss: 0.21252277493476868
Epoch 526, Loss: 0.19534756243228912
Epoch 527, Loss: 0.18928316235542297
Epoch 528, Loss: 0.20300722122192383
Epoch 529, Loss: 0.20482121407985687
Epoch 530, Loss: 0.18926599621772766
Epoch 531, Loss: 0.20433759689331055
Epoch 532, Loss: 0.18850965797901154
Epoch 533, Loss: 0.22325989603996277
Epoch 534, Loss: 0.18275494873523712
Epoch 535, Loss: 0.2024548351764679
Epoch 536, Loss: 0.19071488082408905
Epoch 537, Loss: 0.2086145430803299
Epoch 538, Loss: 0.2010752260684967
Epoch 539, Loss: 0.17948880791664124
Epoch 540, Loss: 0.1996576189994812
Epoch 541, Loss: 0.21086746454238892
Epoch 542, Loss: 0.19713187217712402
Epoch 543, Loss: 0.211943119764328
Epoch 544, Loss: 0.17734795808792114
Epoch 545, Loss: 0.24283115565776825
Epoch 546, Loss: 0.18751715123653412
Epoch 547, Loss: 0.2035616934299469
Epoch 548, Loss: 0.1829109489917755
Epoch 549, Loss: 0.2019122838973999
Epoch 550, Loss: 0.21088509261608124
Epoch 551, Loss: 0.1871824413537979
Epoch 552, Loss: 0.1922612190246582
Epoch 553, Loss: 0.2064392864704132
Epoch 554, Loss: 0.2084522694349289
Epoch 555, Loss: 0.2045300304889679
Epoch 556, Loss: 0.17485472559928894
Epoch 557, Loss: 0.22195571660995483
Epoch 558, Loss: 0.1865350306034088
Epoch 559, Loss: 0.21155598759651184
Epoch 560, Loss: 0.19213993847370148
Epoch 561, Loss: 0.21081297099590302
Epoch 562, Loss: 0.19567374885082245
Epoch 563, Loss: 0.18408584594726562
Epoch 564, Loss: 0.20380830764770508
Epoch 565, Loss: 0.20681829750537872
Epoch 566, Loss: 0.1988043636083603
Epoch 567, Loss: 0.19558309018611908
Epoch 568, Loss: 0.19468370079994202
Epoch 569, Loss: 0.21415965259075165
Epoch 570, Loss: 0.18453505635261536
Epoch 571, Loss: 0.21519820392131805
Epoch 572, Loss: 0.18817386031150818
Epoch 573, Loss: 0.2144542783498764
Epoch 574, Loss: 0.19589129090309143
Epoch 575, Loss: 0.18548059463500977
Epoch 576, Loss: 0.20511938631534576
Epoch 577, Loss: 0.19774742424488068
Epoch 578, Loss: 0.19607840478420258
Epoch 579, Loss: 0.20018252730369568
Epoch 580, Loss: 0.19745874404907227
Epoch 581, Loss: 0.21901053190231323
Epoch 582, Loss: 0.17533712089061737
Epoch 583, Loss: 0.2230547070503235
Epoch 584, Loss: 0.18749122321605682
Epoch 585, Loss: 0.21621587872505188
Epoch 586, Loss: 0.191255584359169
Epoch 587, Loss: 0.17547759413719177
Epoch 588, Loss: 0.20389148592948914
Epoch 589, Loss: 0.19721122086048126
Epoch 590, Loss: 0.1968667060136795
Epoch 591, Loss: 0.20163092017173767
Epoch 592, Loss: 0.19603504240512848
Epoch 593, Loss: 0.20645634829998016
Epoch 594, Loss: 0.18080537021160126
Epoch 595, Loss: 0.208445742726326
Epoch 596, Loss: 0.19482704997062683
Epoch 597, Loss: 0.20857250690460205
Epoch 598, Loss: 0.1950322836637497
Epoch 599, Loss: 0.18601270020008087
Epoch 600, Loss: 0.20965439081192017
Epoch 601, Loss: 0.19443751871585846
Epoch 602, Loss: 0.19362224638462067
Epoch 603, Loss: 0.19325195252895355
Epoch 604, Loss: 0.1942725032567978
Epoch 605, Loss: 0.21130014955997467
Epoch 606, Loss: 0.17260770499706268
Epoch 607, Loss: 0.2211846560239792
Epoch 608, Loss: 0.18699701130390167
Epoch 609, Loss: 0.20805571973323822
Epoch 610, Loss: 0.19076348841190338
Epoch 611, Loss: 0.1766246110200882
Epoch 612, Loss: 0.20347636938095093
Epoch 613, Loss: 0.18731430172920227
Epoch 614, Loss: 0.20179904997348785
Epoch 615, Loss: 0.19805708527565002
Epoch 616, Loss: 0.1988120675086975
Epoch 617, Loss: 0.2005349099636078
Epoch 618, Loss: 0.17909421026706696
Epoch 619, Loss: 0.20716898143291473
Epoch 620, Loss: 0.18598322570323944
Epoch 621, Loss: 0.2047303318977356
Epoch 622, Loss: 0.1941719800233841
Epoch 623, Loss: 0.1874418556690216
Epoch 624, Loss: 0.192581906914711
Epoch 625, Loss: 0.18072494864463806
Epoch 626, Loss: 0.20924177765846252
Epoch 627, Loss: 0.19979815185070038
Epoch 628, Loss: 0.19294556975364685
Epoch 629, Loss: 0.1950467973947525
Epoch 630, Loss: 0.18171480298042297
Epoch 631, Loss: 0.2217431217432022
Epoch 632, Loss: 0.1830001324415207
Epoch 633, Loss: 0.1990465223789215
Epoch 634, Loss: 0.1907140612602234
Epoch 635, Loss: 0.19956324994564056
Epoch 636, Loss: 0.1988234668970108
Epoch 637, Loss: 0.1741768717765808
Epoch 638, Loss: 0.2090994268655777
Epoch 639, Loss: 0.19408287107944489
Epoch 640, Loss: 0.2008248269557953
Epoch 641, Loss: 0.18956956267356873
Epoch 642, Loss: 0.18435247242450714
Epoch 643, Loss: 0.21565237641334534
Epoch 644, Loss: 0.1831948161125183
Epoch 645, Loss: 0.20568031072616577
Epoch 646, Loss: 0.1893678456544876
Epoch 647, Loss: 0.20932616293430328
Epoch 648, Loss: 0.19268745183944702
Epoch 649, Loss: 0.16848884522914886
Epoch 650, Loss: 0.21006707847118378
Epoch 651, Loss: 0.18962152302265167
Epoch 652, Loss: 0.20319488644599915
Epoch 653, Loss: 0.1902560293674469
Epoch 654, Loss: 0.1970192790031433
Epoch 655, Loss: 0.20318932831287384
Epoch 656, Loss: 0.17438550293445587
Epoch 657, Loss: 0.2131580263376236
Epoch 658, Loss: 0.18554840981960297
Epoch 659, Loss: 0.20832832157611847
Epoch 660, Loss: 0.1918145716190338
Epoch 661, Loss: 0.1844697743654251
Epoch 662, Loss: 0.20505960285663605
Epoch 663, Loss: 0.17662617564201355
Epoch 664, Loss: 0.2099301815032959
Epoch 665, Loss: 0.19637204706668854
Epoch 666, Loss: 0.20057840645313263
Epoch 667, Loss: 0.192010760307312
Epoch 668, Loss: 0.17575958371162415
Epoch 669, Loss: 0.21612019836902618
Epoch 670, Loss: 0.18348397314548492
Epoch 671, Loss: 0.20316381752490997
Epoch 672, Loss: 0.1846322864294052
Epoch 673, Loss: 0.19846798479557037
Epoch 674, Loss: 0.19954726099967957
Epoch 675, Loss: 0.1761569231748581
Epoch 676, Loss: 0.20031960308551788
Epoch 677, Loss: 0.1926293522119522
Epoch 678, Loss: 0.20028391480445862
Epoch 679, Loss: 0.18797382712364197
Epoch 680, Loss: 0.1895080804824829
Epoch 681, Loss: 0.20299474895000458
Epoch 682, Loss: 0.1797601580619812
Epoch 683, Loss: 0.19516371190547943
Epoch 684, Loss: 0.18724478781223297
Epoch 685, Loss: 0.2036154866218567
Epoch 686, Loss: 0.19333721697330475
Epoch 687, Loss: 0.17414110898971558
Epoch 688, Loss: 0.20446495711803436
Epoch 689, Loss: 0.18107248842716217
Epoch 690, Loss: 0.1999073028564453
Epoch 691, Loss: 0.19161728024482727
Epoch 692, Loss: 0.19578233361244202
Epoch 693, Loss: 0.20180165767669678
Epoch 694, Loss: 0.17084267735481262
Epoch 695, Loss: 0.21482768654823303
Epoch 696, Loss: 0.1810295134782791
Epoch 697, Loss: 0.19569890201091766
Epoch 698, Loss: 0.18359997868537903
Epoch 699, Loss: 0.1790478527545929
Epoch 700, Loss: 0.21397805213928223
Epoch 701, Loss: 0.17905017733573914
Epoch 702, Loss: 0.19898846745491028
Epoch 703, Loss: 0.18623554706573486
Epoch 704, Loss: 0.20445477962493896
Epoch 705, Loss: 0.20018889009952545
Epoch 706, Loss: 0.17023880779743195
Epoch 707, Loss: 0.2058364748954773
Epoch 708, Loss: 0.18141557276248932
Epoch 709, Loss: 0.21126502752304077
Epoch 710, Loss: 0.18668891489505768
Epoch 711, Loss: 0.18402335047721863
Epoch 712, Loss: 0.2064433991909027
Epoch 713, Loss: 0.17515239119529724
Epoch 714, Loss: 0.21130965650081635
Epoch 715, Loss: 0.18679876625537872
Epoch 716, Loss: 0.20463909208774567
Epoch 717, Loss: 0.19354036450386047
Epoch 718, Loss: 0.17919179797172546
Epoch 719, Loss: 0.20498184859752655
Epoch 720, Loss: 0.17885440587997437
Epoch 721, Loss: 0.2165880650281906
Epoch 722, Loss: 0.18719935417175293
Epoch 723, Loss: 0.1943603754043579
Epoch 724, Loss: 0.19722993671894073
Epoch 725, Loss: 0.17461518943309784
Epoch 726, Loss: 0.20889663696289062
Epoch 727, Loss: 0.18265685439109802
Epoch 728, Loss: 0.20063064992427826
Epoch 729, Loss: 0.1936877965927124
Epoch 730, Loss: 0.19506123661994934
Epoch 731, Loss: 0.19507282972335815
Epoch 732, Loss: 0.17038294672966003
Epoch 733, Loss: 0.21146360039710999
Epoch 734, Loss: 0.1903478354215622
Epoch 735, Loss: 0.19278419017791748
Epoch 736, Loss: 0.19451367855072021
Epoch 737, Loss: 0.17895586788654327
Epoch 738, Loss: 0.20663760602474213
Epoch 739, Loss: 0.1844189614057541
Epoch 740, Loss: 0.20517155528068542
Epoch 741, Loss: 0.18933285772800446
Epoch 742, Loss: 0.19144931435585022
Epoch 743, Loss: 0.19920726120471954
Epoch 744, Loss: 0.17206043004989624
Epoch 745, Loss: 0.21511678397655487
Epoch 746, Loss: 0.1878174990415573
Epoch 747, Loss: 0.2002754956483841
Epoch 748, Loss: 0.18970856070518494
Epoch 749, Loss: 0.17604473233222961
Epoch 750, Loss: 0.21385887265205383
Epoch 751, Loss: 0.18038849532604218
Epoch 752, Loss: 0.20574453473091125
Epoch 753, Loss: 0.18768596649169922
Epoch 754, Loss: 0.20236515998840332
Epoch 755, Loss: 0.19997593760490417
Epoch 756, Loss: 0.16819745302200317
Epoch 757, Loss: 0.2113630771636963
Epoch 758, Loss: 0.18414273858070374
Epoch 759, Loss: 0.20592676103115082
Epoch 760, Loss: 0.1895371824502945
Epoch 761, Loss: 0.19161789119243622
Epoch 762, Loss: 0.20141349732875824
Epoch 763, Loss: 0.17281006276607513
Epoch 764, Loss: 0.21162593364715576
Epoch 765, Loss: 0.18742896616458893
Epoch 766, Loss: 0.20006321370601654
Epoch 767, Loss: 0.19438965618610382
Epoch 768, Loss: 0.17813600599765778
Epoch 769, Loss: 0.20831865072250366
Epoch 770, Loss: 0.17931681871414185
Epoch 771, Loss: 0.20582683384418488
Epoch 772, Loss: 0.19174592196941376
Epoch 773, Loss: 0.20342987775802612
Epoch 774, Loss: 0.19530297815799713
Epoch 775, Loss: 0.16574110090732574
Epoch 776, Loss: 0.2212122529745102
Epoch 777, Loss: 0.18328358232975006
Epoch 778, Loss: 0.1906524896621704
Epoch 779, Loss: 0.1934918612241745
Epoch 780, Loss: 0.2011118084192276
Epoch 781, Loss: 0.19396591186523438
Epoch 782, Loss: 0.17214766144752502
Epoch 783, Loss: 0.2158888429403305
Epoch 784, Loss: 0.19408638775348663
Epoch 785, Loss: 0.18458659946918488
Epoch 786, Loss: 0.18685925006866455
Epoch 787, Loss: 0.19229434430599213
Epoch 788, Loss: 0.20438705384731293
Epoch 789, Loss: 0.18068543076515198
Epoch 790, Loss: 0.1937197595834732
Epoch 791, Loss: 0.1901397705078125
Epoch 792, Loss: 0.19923876225948334
Epoch 793, Loss: 0.19041530787944794
Epoch 794, Loss: 0.17465226352214813
Epoch 795, Loss: 0.20658119022846222
Epoch 796, Loss: 0.18805477023124695
Epoch 797, Loss: 0.19697332382202148
Epoch 798, Loss: 0.18928252160549164
Epoch 799, Loss: 0.19223077595233917
Epoch 800, Loss: 0.20516689121723175
Epoch 801, Loss: 0.17059041559696198
Epoch 802, Loss: 0.21176956593990326
Epoch 803, Loss: 0.1899767965078354
Epoch 804, Loss: 0.20244216918945312
Epoch 805, Loss: 0.19141808152198792
Epoch 806, Loss: 0.17257899045944214
Epoch 807, Loss: 0.21871070563793182
Epoch 808, Loss: 0.17910201847553253
Epoch 809, Loss: 0.19759248197078705
Epoch 810, Loss: 0.18539807200431824
Epoch 811, Loss: 0.20138207077980042
Epoch 812, Loss: 0.20202858746051788
Epoch 813, Loss: 0.16791534423828125
Epoch 814, Loss: 0.2121216505765915
Epoch 815, Loss: 0.18266987800598145
Epoch 816, Loss: 0.20918667316436768
Epoch 817, Loss: 0.189771369099617
Epoch 818, Loss: 0.18394730985164642
Epoch 819, Loss: 0.21013972163200378
Epoch 820, Loss: 0.17721660435199738
Epoch 821, Loss: 0.21200597286224365
Epoch 822, Loss: 0.18605992197990417
Epoch 823, Loss: 0.20668284595012665
Epoch 824, Loss: 0.19975616037845612
Epoch 825, Loss: 0.17038840055465698
Epoch 826, Loss: 0.2098139077425003
Epoch 827, Loss: 0.17935167253017426
Epoch 828, Loss: 0.2092658132314682
Epoch 829, Loss: 0.18981721997261047
Epoch 830, Loss: 0.2003752440214157
Epoch 831, Loss: 0.1970471441745758
Epoch 832, Loss: 0.16728968918323517
Epoch 833, Loss: 0.21487852931022644
Epoch 834, Loss: 0.18447266519069672
Epoch 835, Loss: 0.19959554076194763
Epoch 836, Loss: 0.19152827560901642
Epoch 837, Loss: 0.19670236110687256
Epoch 838, Loss: 0.20138610899448395
Epoch 839, Loss: 0.17334279417991638
Epoch 840, Loss: 0.20792607963085175
Epoch 841, Loss: 0.1909835785627365
Epoch 842, Loss: 0.19544212520122528
Epoch 843, Loss: 0.18960893154144287
Epoch 844, Loss: 0.18173132836818695
Epoch 845, Loss: 0.2046474665403366
Epoch 846, Loss: 0.18391568958759308
Epoch 847, Loss: 0.19878032803535461
Epoch 848, Loss: 0.19100730121135712
Epoch 849, Loss: 0.1931590735912323
Epoch 850, Loss: 0.1951070874929428
Epoch 851, Loss: 0.17289240658283234
Epoch 852, Loss: 0.21388079226016998
Epoch 853, Loss: 0.18575328588485718
Epoch 854, Loss: 0.19110718369483948
Epoch 855, Loss: 0.1920657753944397
Epoch 856, Loss: 0.18083541095256805
Epoch 857, Loss: 0.20216569304466248
Epoch 858, Loss: 0.17577406764030457
Epoch 859, Loss: 0.21234948933124542
Epoch 860, Loss: 0.1870453655719757
Epoch 861, Loss: 0.1897866427898407
Epoch 862, Loss: 0.18966403603553772
Epoch 863, Loss: 0.17474554479122162
Epoch 864, Loss: 0.20050767064094543
Epoch 865, Loss: 0.1850375086069107
Epoch 866, Loss: 0.19302529096603394
Epoch 867, Loss: 0.18899665772914886
Epoch 868, Loss: 0.189809188246727
Epoch 869, Loss: 0.19490879774093628
Epoch 870, Loss: 0.17696724832057953
Epoch 871, Loss: 0.19933050870895386
Epoch 872, Loss: 0.17829158902168274
Epoch 873, Loss: 0.19215331971645355
Epoch 874, Loss: 0.19237984716892242
Epoch 875, Loss: 0.1759507656097412
Epoch 876, Loss: 0.20253077149391174
Epoch 877, Loss: 0.17496928572654724
Epoch 878, Loss: 0.2063542902469635
Epoch 879, Loss: 0.18383373320102692
Epoch 880, Loss: 0.1975705623626709
Epoch 881, Loss: 0.20090743899345398
Epoch 882, Loss: 0.1665288507938385
Epoch 883, Loss: 0.21460723876953125
Epoch 884, Loss: 0.17450939118862152
Epoch 885, Loss: 0.20682288706302643
Epoch 886, Loss: 0.18273428082466125
Epoch 887, Loss: 0.19276200234889984
Epoch 888, Loss: 0.2033083289861679
Epoch 889, Loss: 0.1706819385290146
Epoch 890, Loss: 0.21076513826847076
Epoch 891, Loss: 0.17524933815002441
Epoch 892, Loss: 0.20763540267944336
Epoch 893, Loss: 0.19024258852005005
Epoch 894, Loss: 0.18422570824623108
Epoch 895, Loss: 0.2108500599861145
Epoch 896, Loss: 0.17303220927715302
Epoch 897, Loss: 0.20993071794509888
Epoch 898, Loss: 0.18072091042995453
Epoch 899, Loss: 0.20265044271945953
Epoch 900, Loss: 0.20061106979846954
Epoch 901, Loss: 0.178780198097229
Epoch 902, Loss: 0.1997053325176239
Epoch 903, Loss: 0.1746700406074524
Epoch 904, Loss: 0.2171817272901535
Epoch 905, Loss: 0.18510304391384125
Epoch 906, Loss: 0.18761670589447021
Epoch 907, Loss: 0.19452890753746033
Epoch 908, Loss: 0.17517341673374176
Epoch 909, Loss: 0.20105572044849396
Epoch 910, Loss: 0.1814609169960022
Epoch 911, Loss: 0.2090054750442505
Epoch 912, Loss: 0.18815885484218597
Epoch 913, Loss: 0.19025187194347382
Epoch 914, Loss: 0.18891113996505737
Epoch 915, Loss: 0.17103715240955353
Epoch 916, Loss: 0.21188440918922424
Epoch 917, Loss: 0.18441778421401978
Epoch 918, Loss: 0.18950149416923523
Epoch 919, Loss: 0.18706656992435455
Epoch 920, Loss: 0.18906112015247345
Epoch 921, Loss: 0.20065176486968994
Epoch 922, Loss: 0.1757918894290924
Epoch 923, Loss: 0.20692729949951172
Epoch 924, Loss: 0.18539035320281982
Epoch 925, Loss: 0.1891358345746994
Epoch 926, Loss: 0.19145120680332184
Epoch 927, Loss: 0.1782594621181488
Epoch 928, Loss: 0.20577898621559143
Epoch 929, Loss: 0.17817428708076477
Epoch 930, Loss: 0.19787243008613586
Epoch 931, Loss: 0.18667006492614746
Epoch 932, Loss: 0.19086968898773193
Epoch 933, Loss: 0.20202110707759857
Epoch 934, Loss: 0.16609688103199005
Epoch 935, Loss: 0.20887243747711182
Epoch 936, Loss: 0.1807502806186676
Epoch 937, Loss: 0.20234498381614685
Epoch 938, Loss: 0.1960325688123703
Epoch 939, Loss: 0.17322562634944916
Epoch 940, Loss: 0.20866477489471436
Epoch 941, Loss: 0.1778181493282318
Epoch 942, Loss: 0.2109142392873764
Epoch 943, Loss: 0.18119655549526215
Epoch 944, Loss: 0.2022809088230133
Epoch 945, Loss: 0.20011299848556519
Epoch 946, Loss: 0.16746723651885986
Epoch 947, Loss: 0.2161901295185089
Epoch 948, Loss: 0.17654496431350708
Epoch 949, Loss: 0.20782557129859924
Epoch 950, Loss: 0.18352508544921875
Epoch 951, Loss: 0.19850674271583557
Epoch 952, Loss: 0.19985979795455933
Epoch 953, Loss: 0.16582097113132477
Epoch 954, Loss: 0.21152126789093018
Epoch 955, Loss: 0.18136045336723328
Epoch 956, Loss: 0.19594356417655945
Epoch 957, Loss: 0.1956220418214798
Epoch 958, Loss: 0.1911388337612152
Epoch 959, Loss: 0.19344419240951538
Epoch 960, Loss: 0.17557327449321747
Epoch 961, Loss: 0.2117593139410019
Epoch 962, Loss: 0.1856854110956192
Epoch 963, Loss: 0.18373598158359528
Epoch 964, Loss: 0.19162218272686005
Epoch 965, Loss: 0.1858338862657547
Epoch 966, Loss: 0.19919180870056152
Epoch 967, Loss: 0.1776212751865387
Epoch 968, Loss: 0.20269526541233063
Epoch 969, Loss: 0.18983496725559235
Epoch 970, Loss: 0.19268837571144104
Epoch 971, Loss: 0.1922241896390915
Epoch 972, Loss: 0.16973087191581726
Epoch 973, Loss: 0.21310631930828094
Epoch 974, Loss: 0.18357253074645996
Epoch 975, Loss: 0.1923273503780365
Epoch 976, Loss: 0.1871584951877594
Epoch 977, Loss: 0.18466994166374207
Epoch 978, Loss: 0.20626512169837952
Epoch 979, Loss: 0.17703545093536377
Epoch 980, Loss: 0.20152029395103455
Epoch 981, Loss: 0.1833394169807434
Epoch 982, Loss: 0.19885246455669403
Epoch 983, Loss: 0.1915104240179062
Epoch 984, Loss: 0.1707518845796585
Epoch 985, Loss: 0.20459505915641785
Epoch 986, Loss: 0.1782873570919037
Epoch 987, Loss: 0.20082254707813263
Epoch 988, Loss: 0.18611904978752136
Epoch 989, Loss: 0.19171512126922607
Epoch 990, Loss: 0.1982295960187912
Epoch 991, Loss: 0.1717178076505661
Epoch 992, Loss: 0.2058137208223343
Epoch 993, Loss: 0.1791379153728485
Epoch 994, Loss: 0.19762837886810303
Epoch 995, Loss: 0.1920527070760727
Epoch 996, Loss: 0.17960339784622192
Epoch 997, Loss: 0.20878176391124725
Epoch 998, Loss: 0.1760302484035492
Epoch 999, Loss: 0.20544934272766113
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">mse_initial</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X_train_scaled</span> <span class="o">-</span> <span class="n">y_train_scaled</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span>  <span class="n">losses</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">mse_initial</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Initial MSE&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Loss&#39;)
</pre></div>
</div>
<img alt="../_images/d00af783173e61a83e065fcfa2b8a582c27203fd2fdc1258f1f7f7bd414398ac.png" src="../_images/d00af783173e61a83e065fcfa2b8a582c27203fd2fdc1258f1f7f7bd414398ac.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># randomly select 4 waves to visualise</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y_pred_scaled</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_scaled</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler_x</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_test_scaled</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">y</span><span class="o">=</span><span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;predicted&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">y</span><span class="o">=</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;degraded&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;original&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b8a0eb95f837efa603eb04a61bba4a6cc6a84060b4a2ec5fd5f32907dc674b01.png" src="../_images/b8a0eb95f837efa603eb04a61bba4a6cc6a84060b4a2ec5fd5f32907dc674b01.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Week7"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theory">Theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-test-data">Generate test data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-data">Preparing the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-mlp-model">Defining the MLP model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#initialize-model">Initialize model</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Malachi Hibbins
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>